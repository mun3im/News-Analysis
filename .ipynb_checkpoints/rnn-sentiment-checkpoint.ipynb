{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = unidecode(string)\n",
    "    string = re.sub('[^A-Za-z ]+', '', string)\n",
    "    string = word_tokenize(string)\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = [y for y in string if len(y) > 2 and y.find('nbsp') < 0 and y.find('href') < 0 and y not in english_stopwords]\n",
    "    string = ' '.join(string).lower()\n",
    "    return ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "\n",
    "def read_data(location):\n",
    "    list_folder = os.listdir(location)\n",
    "    label = list_folder\n",
    "    label.sort()\n",
    "    outer_string, outer_label = [], []\n",
    "    for i in range(len(list_folder)):\n",
    "        list_file = os.listdir(location + list_folder[i])\n",
    "        strings = []\n",
    "        for x in range(len(list_file)):\n",
    "            with open(location + list_folder[i] + '/' + list_file[x], 'r') as fopen:\n",
    "                strings += fopen.read().split('\\n')\n",
    "        strings = list(filter(None, strings))\n",
    "        for k in range(len(strings)):\n",
    "            strings[k] = clearstring(strings[k])\n",
    "        labels = [i] * len(strings)\n",
    "        outer_string += strings\n",
    "        outer_label += labels\n",
    "    \n",
    "    dataset = np.array([outer_string, outer_label])\n",
    "    dataset = dataset.T\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    string = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        string += dataset[i][0].split()\n",
    "    \n",
    "    return string, dataset, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 'bland inert production one shakespeare vibrant plays guess intent make play accessible understandable possible audience exposed shakespeare though making every line clear every intent obvious drained play life turned flat caricature somehow actually boring hard feat given wonderful material acting forgettable best sam waterston benedick douglas watson pedro others however fare well april shawnham hero pouty breathless airhead frequently provokes winces jerry mayer john nonsensical cartoon character level snidely whiplash though snidley much enjoyable murray abraham know guy killed mozart version unless disguise name removed credits given producer joseph papp basically theater god production disappointing head scratching well bother watch branagh much ado instead version overflowing vitality humor say nothing wonderful performances',\n",
       "        '0'],\n",
       "       [ 'german private ill renowned copying dutch naturally formats well case edel starck xeroxing went far basics screwball stand screwball comedy watch edel starck seriously expect yet another lawyers drama thingy similar laworder something well somewhere else watch laworder quite brilliant digress funny often addresses thought provoking themes funny romantic funny funny witty frankly quite dismayed writers get better deals serial final cynical nature needs readjust private german television productions german residents understand talking short watch foggiest english synchronisation like hey worth learning german watch four seasons pseudo happy end included',\n",
       "        '1'],\n",
       "       [ 'movie stinks stench resembles bad cowpies sat sun long believe many talented actors wasted time making hopelessly awful film whew',\n",
       "        '0'],\n",
       "       [ 'great movie sow people think might based true story matter movie great think balloon mermaid ends flying mermaid town instead thinking little girl wish came true means peaceful dreams come true trust world make true little girl desi movie mom best actors seen long time good actors director someone tell please tell year guy romania says thank making movie',\n",
       "        '1'],\n",
       "       [ 'likely voted best comedy year many coincidences plot holes however talking movie hit man white bread salesman become buddies vagaries come much surprise brosnan excellent role gone wooden james bond role wasted maintain kind quality hope continues make comedies greg kinnear also excellent brosnan straight man read negative comments hope davis thought quite good mousy housewife dark side buried deep within lots good chuckles brosnan sleazes way scenes nearly died laughing father consultant nearly lost julian describes facilitator much like grosse pointe blank another hit man comedy humour dark prepared enjoy',\n",
       "        '1']],\n",
       "      dtype='<U9081')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,df,label = read_data('/home/husein/space/text-dataset/sentiment/data/')\n",
    "df[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector-sentiment.p', 'rb') as fopen:\n",
    "    vectors = pickle.load(fopen)\n",
    "with open('dict-sentiment.p', 'rb') as fopen:\n",
    "    dictionary = pickle.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(df[:,0], df[:, 1].astype('int'), test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer)\n",
    "        self.rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(self.rnn_cells, output_keep_prob = 0.5)\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, dtype = tf.float32)\n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n",
    "        self.logits = tf.matmul(self.outputs[:, -1], self.rnn_W) + self.rnn_B\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        l2 = sum(0.0005 * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        self.cost += l2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(df[i,0].split())for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.961"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 60\n",
    "location = os.getcwd()\n",
    "num_layers = 2\n",
    "size_layer = 256\n",
    "learning_rate = 0.0001\n",
    "batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-b7c083deade6>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "epoch: 0 , pass acc: 0 , current acc: 0.611799997091\n",
      "time taken: 26.790687561035156\n",
      "epoch: 1 , training loss: 1.26772443533 , training acc: 0.578849998564 , valid loss: 1.1737433815 , valid acc: 0.611799997091\n",
      "epoch: 1 , pass acc: 0.611799997091 , current acc: 0.68400000453\n",
      "time taken: 26.385164976119995\n",
      "epoch: 2 , training loss: 1.11333983958 , training acc: 0.659199998677 , valid loss: 1.07287054181 , valid acc: 0.68400000453\n",
      "epoch: 2 , pass acc: 0.68400000453 , current acc: 0.699800001383\n",
      "time taken: 26.471251726150513\n",
      "epoch: 3 , training loss: 1.02642693698 , training acc: 0.704400001764 , valid loss: 1.01873482704 , valid acc: 0.699800001383\n",
      "epoch: 3 , pass acc: 0.699800001383 , current acc: 0.705400002003\n",
      "time taken: 26.53897476196289\n",
      "epoch: 4 , training loss: 0.982846246958 , training acc: 0.725049999356 , valid loss: 0.990427957773 , valid acc: 0.705400002003\n",
      "epoch: 4 , pass acc: 0.705400002003 , current acc: 0.720200002193\n",
      "time taken: 26.573443174362183\n",
      "epoch: 5 , training loss: 0.933874602914 , training acc: 0.743750000894 , valid loss: 0.95317545414 , valid acc: 0.720200002193\n",
      "epoch: 5 , pass acc: 0.720200002193 , current acc: 0.730800001621\n",
      "time taken: 26.571908712387085\n",
      "epoch: 6 , training loss: 0.900874506533 , training acc: 0.754049997926 , valid loss: 0.92660589695 , valid acc: 0.730800001621\n",
      "epoch: 6 , pass acc: 0.730800001621 , current acc: 0.741000000238\n",
      "time taken: 26.56037163734436\n",
      "epoch: 7 , training loss: 0.857034261227 , training acc: 0.769449998736 , valid loss: 0.901009935141 , valid acc: 0.741000000238\n",
      "time taken: 26.489935398101807\n",
      "epoch: 8 , training loss: 0.827828482687 , training acc: 0.780599998832 , valid loss: 0.88943076849 , valid acc: 0.736199998856\n",
      "epoch: 8 , pass acc: 0.741000000238 , current acc: 0.749200000763\n",
      "time taken: 26.559102773666382\n",
      "epoch: 9 , training loss: 0.78868085444 , training acc: 0.801099996269 , valid loss: 0.860827020407 , valid acc: 0.749200000763\n",
      "epoch: 9 , pass acc: 0.749200000763 , current acc: 0.764399998188\n",
      "time taken: 26.597166061401367\n",
      "epoch: 10 , training loss: 0.752564097047 , training acc: 0.815499997735 , valid loss: 0.845876865387 , valid acc: 0.764399998188\n",
      "epoch: 10 , pass acc: 0.764399998188 , current acc: 0.764600001574\n",
      "time taken: 26.5887553691864\n",
      "epoch: 11 , training loss: 0.745022690296 , training acc: 0.814249997735 , valid loss: 0.836280465126 , valid acc: 0.764600001574\n",
      "epoch: 11 , pass acc: 0.764600001574 , current acc: 0.77659999609\n",
      "time taken: 26.596354722976685\n",
      "epoch: 12 , training loss: 0.699992431998 , training acc: 0.838399999142 , valid loss: 0.810480712652 , valid acc: 0.77659999609\n",
      "epoch: 12 , pass acc: 0.77659999609 , current acc: 0.783199999332\n",
      "time taken: 26.58343243598938\n",
      "epoch: 13 , training loss: 0.641300233603 , training acc: 0.865150001347 , valid loss: 0.802602562904 , valid acc: 0.783199999332\n",
      "time taken: 26.526774168014526\n",
      "epoch: 14 , training loss: 0.63515473038 , training acc: 0.865049999356 , valid loss: 0.816333686113 , valid acc: 0.780799998045\n",
      "time taken: 26.521199226379395\n",
      "epoch: 15 , training loss: 0.583223293126 , training acc: 0.886550001502 , valid loss: 0.842642827034 , valid acc: 0.76840000391\n",
      "time taken: 26.519329071044922\n",
      "epoch: 16 , training loss: 0.602190016955 , training acc: 0.875600000918 , valid loss: 0.810683754683 , valid acc: 0.778199994564\n",
      "epoch: 16 , pass acc: 0.783199999332 , current acc: 0.787199997902\n",
      "time taken: 26.583683967590332\n",
      "epoch: 17 , training loss: 0.559449064881 , training acc: 0.898250003755 , valid loss: 0.817571513653 , valid acc: 0.787199997902\n",
      "epoch: 17 , pass acc: 0.787199997902 , current acc: 0.78879999876\n",
      "time taken: 26.566697359085083\n",
      "epoch: 18 , training loss: 0.53580878064 , training acc: 0.90535000205 , valid loss: 0.821450428963 , valid acc: 0.78879999876\n",
      "time taken: 26.527301788330078\n",
      "epoch: 19 , training loss: 0.484240622073 , training acc: 0.929500001967 , valid loss: 0.859520198107 , valid acc: 0.783800004721\n",
      "time taken: 26.531289100646973\n",
      "epoch: 20 , training loss: 0.458980500698 , training acc: 0.93810000211 , valid loss: 0.897757940292 , valid acc: 0.784599995613\n",
      "epoch: 20 , pass acc: 0.78879999876 , current acc: 0.791399997473\n",
      "time taken: 26.612117290496826\n",
      "epoch: 21 , training loss: 0.499295016974 , training acc: 0.920350002646 , valid loss: 0.820691934824 , valid acc: 0.791399997473\n",
      "time taken: 26.539732217788696\n",
      "epoch: 22 , training loss: 0.474660877585 , training acc: 0.932550001442 , valid loss: 0.886259372234 , valid acc: 0.788199996948\n",
      "epoch: 22 , pass acc: 0.791399997473 , current acc: 0.793399999142\n",
      "time taken: 26.60496497154236\n",
      "epoch: 23 , training loss: 0.442473600656 , training acc: 0.942600004077 , valid loss: 0.919586769342 , valid acc: 0.793399999142\n",
      "time taken: 26.553609371185303\n",
      "epoch: 24 , training loss: 0.427500397861 , training acc: 0.950950006545 , valid loss: 1.01734910846 , valid acc: 0.786800003052\n",
      "time taken: 26.567609548568726\n",
      "epoch: 25 , training loss: 0.428700836003 , training acc: 0.947950006127 , valid loss: 1.02817936301 , valid acc: 0.786399998665\n",
      "time taken: 26.61938190460205\n",
      "epoch: 26 , training loss: 0.409296827912 , training acc: 0.958500005901 , valid loss: 1.01814250469 , valid acc: 0.785999999046\n",
      "time taken: 26.737542390823364\n",
      "epoch: 27 , training loss: 0.348023447543 , training acc: 0.979950008094 , valid loss: 1.15125849128 , valid acc: 0.777399992943\n",
      "time taken: 26.72038984298706\n",
      "epoch: 28 , training loss: 0.350254948437 , training acc: 0.977600005567 , valid loss: 1.05254877329 , valid acc: 0.788599997759\n",
      "time taken: 30.351978540420532\n",
      "epoch: 29 , training loss: 0.365463433862 , training acc: 0.972950009704 , valid loss: 1.15400897741 , valid acc: 0.782999995947\n",
      "time taken: 44.07647204399109\n",
      "epoch: 30 , training loss: 0.354575068653 , training acc: 0.975850005746 , valid loss: 1.23084649801 , valid acc: 0.771000000238\n",
      "time taken: 46.924468755722046\n",
      "epoch: 31 , training loss: 0.407540510446 , training acc: 0.955600005686 , valid loss: 1.17292837262 , valid acc: 0.756000001431\n",
      "time taken: 46.88483381271362\n",
      "epoch: 32 , training loss: 0.398260433972 , training acc: 0.960100004375 , valid loss: 1.09507278562 , valid acc: 0.766799998283\n",
      "time taken: 46.98239350318909\n",
      "epoch: 33 , training loss: 0.356979626119 , training acc: 0.973700006008 , valid loss: 1.07945267081 , valid acc: 0.776199997663\n",
      "break epoch: 33\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, vectors.shape[1], len(label), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dimension = vectors.shape[1]\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 10, 0, 0, 0\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (train_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = train_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            batch_y[k, int(train_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        train_loss += loss\n",
    "        train_acc += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    \n",
    "    for i in range(0, (test_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = test_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(test_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, acc = sess.run([model.cost, model.accuracy], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        \n",
    "    train_loss /= (train_X.shape[0] // batch)\n",
    "    train_acc /= (train_X.shape[0] // batch)\n",
    "    test_loss /= (test_X.shape[0] // batch)\n",
    "    test_acc /= (test_X.shape[0] // batch)\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc)\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        saver.save(sess, os.getcwd() + \"/rnn-sentiment.ckpt\")\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1\n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc, ', valid loss:', test_loss, ', valid acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
