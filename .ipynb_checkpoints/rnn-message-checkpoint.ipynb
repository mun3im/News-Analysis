{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "import time\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearstring(string):\n",
    "    string = unidecode(string)\n",
    "    string = re.sub('[^A-Za-z ]+', '', string)\n",
    "    string = word_tokenize(string)\n",
    "    string = filter(None, string)\n",
    "    string = [y.strip() for y in string]\n",
    "    string = [y for y in string if len(y) > 2 and y.find('nbsp') < 0 and y.find('href') < 0 and y not in english_stopwords]\n",
    "    string = ' '.join(string).lower()\n",
    "    return ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "\n",
    "def read_data(location):\n",
    "    list_folder = os.listdir(location)\n",
    "    label = list_folder\n",
    "    label.sort()\n",
    "    outer_string, outer_label = [], []\n",
    "    for i in range(len(list_folder)):\n",
    "        list_file = os.listdir(location + list_folder[i])\n",
    "        strings = []\n",
    "        for x in range(len(list_file)):\n",
    "            with open(location + list_folder[i] + '/' + list_file[x], 'r') as fopen:\n",
    "                strings += fopen.read().split('\\n')\n",
    "        strings = list(filter(None, strings))\n",
    "        for k in range(len(strings)):\n",
    "            strings[k] = clearstring(strings[k])\n",
    "        labels = [i] * len(strings)\n",
    "        outer_string += strings\n",
    "        outer_label += labels\n",
    "    \n",
    "    dataset = np.array([outer_string, outer_label])\n",
    "    dataset = dataset.T\n",
    "    np.random.shuffle(dataset)\n",
    "    \n",
    "    string = []\n",
    "    for i in range(dataset.shape[0]):\n",
    "        string += dataset[i][0].split()\n",
    "    \n",
    "    return string, dataset, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 'statement congressman danny davis military intervention syria apparent use chemical weapons syria human tragedy hearts families lost lives much remains unknown events today congress beginning debate issue constituents deeply concerned events syria searching appropriate response end killing move quickly toward political solution war syria however overwhelmingly opposed military intervention kind concerned morality action concerned legality action concerned loss life resulting action concerned effectiveness lack effectiveness action concerned action involving deeply civil war syria concerned follow conflict action might trigger concerned sabotaging possible agreement iran nuclear weapons concerned drawing resources focus critical immediate problems home share concerns remain today staunch support president obama however first responsibility member congress represent interests views constituents calls emails letters forms communication office running opposition military intervention syria time therefore strongly leaning toward vote military intervention intend continue listen closely president obamas statements views constituents debate proceeds',\n",
       "        '7'],\n",
       "       [ 'thrilled groundbreaking expansion increase mobility safety jobs minnesota',\n",
       "        '1'],\n",
       "       [ 'degrees received today symbolize willingness amp responsibility leading future yorkcollegecuny',\n",
       "        '6'],\n",
       "       [ 'repmartharoby proud join friends repcorygardner reprichhudson repkevinyoder reptimgriffin amp repmgriffith discuss aff',\n",
       "        '2'],\n",
       "       ['planning date apple eye apple orchards choose harvard dthursday',\n",
       "        '6']],\n",
       "      dtype='<U8069')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,df,label = read_data('/home/husein/space/text-dataset/message/data/')\n",
    "df[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector-message.p', 'rb') as fopen:\n",
    "    vectors = pickle.load(fopen)\n",
    "with open('dict-message.p', 'rb') as fopen:\n",
    "    dictionary = pickle.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(df[:,0], df[:, 1].astype('int'), test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n",
    "        def lstm_cell():\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer)\n",
    "        self.rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n",
    "        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(self.rnn_cells, output_keep_prob = 0.5)\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(drop, self.X, dtype = tf.float32)\n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n",
    "        self.logits = tf.matmul(self.outputs[:, -1], self.rnn_W) + self.rnn_B\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n",
    "        l2 = sum(0.0005 * tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        self.cost += l2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n",
    "        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(df[i,0].split())for i in range(df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.80012016823553"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "location = os.getcwd()\n",
    "num_layers = 2\n",
    "size_layer = 256\n",
    "learning_rate = 0.0001\n",
    "batch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-b7c083deade6>:14: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "epoch: 0 , pass acc: 0 , current acc: 0.252222221759\n",
      "time taken: 3.4906442165374756\n",
      "epoch: 1 , training loss: 3.15088228079 , training acc: 0.254102564393 , valid loss: 3.01215646002 , valid acc: 0.252222221759\n",
      "epoch: 1 , pass acc: 0.252222221759 , current acc: 0.282222222951\n",
      "time taken: 3.2484922409057617\n",
      "epoch: 2 , training loss: 2.82656818781 , training acc: 0.312564102503 , valid loss: 2.93745064735 , valid acc: 0.282222222951\n",
      "time taken: 3.1713643074035645\n",
      "epoch: 3 , training loss: 2.72776541954 , training acc: 0.358717949727 , valid loss: 2.85411951277 , valid acc: 0.276666667726\n",
      "epoch: 3 , pass acc: 0.282222222951 , current acc: 0.294444446762\n",
      "time taken: 3.2841989994049072\n",
      "epoch: 4 , training loss: 2.63015687771 , training acc: 0.37307692109 , valid loss: 2.85543113285 , valid acc: 0.294444446762\n",
      "epoch: 4 , pass acc: 0.294444446762 , current acc: 0.316666666004\n",
      "time taken: 3.2802090644836426\n",
      "epoch: 5 , training loss: 2.58006078158 , training acc: 0.395641026589 , valid loss: 2.7926446067 , valid acc: 0.316666666004\n",
      "time taken: 3.215050458908081\n",
      "epoch: 6 , training loss: 2.52547423656 , training acc: 0.411282050304 , valid loss: 2.80433124966 , valid acc: 0.310000002384\n",
      "epoch: 6 , pass acc: 0.316666666004 , current acc: 0.32555555635\n",
      "time taken: 3.262733221054077\n",
      "epoch: 7 , training loss: 2.4756722328 , training acc: 0.425897433972 , valid loss: 2.77652186818 , valid acc: 0.32555555635\n",
      "time taken: 3.1812381744384766\n",
      "epoch: 8 , training loss: 2.43635888589 , training acc: 0.437692306745 , valid loss: 2.80201527807 , valid acc: 0.314444450869\n",
      "time taken: 3.166839361190796\n",
      "epoch: 9 , training loss: 2.38400639632 , training acc: 0.453076918156 , valid loss: 2.80833742354 , valid acc: 0.32555555635\n",
      "epoch: 9 , pass acc: 0.32555555635 , current acc: 0.329999999868\n",
      "time taken: 3.2550156116485596\n",
      "epoch: 10 , training loss: 2.33974878605 , training acc: 0.452307687356 , valid loss: 2.80795235104 , valid acc: 0.329999999868\n",
      "time taken: 3.1763477325439453\n",
      "epoch: 11 , training loss: 2.30977000334 , training acc: 0.473846151279 , valid loss: 2.80816406674 , valid acc: 0.304444445504\n",
      "epoch: 11 , pass acc: 0.329999999868 , current acc: 0.332222223282\n",
      "time taken: 3.025158405303955\n",
      "epoch: 12 , training loss: 2.27147717965 , training acc: 0.488461537239 , valid loss: 2.8073999087 , valid acc: 0.332222223282\n",
      "time taken: 2.716456174850464\n",
      "epoch: 13 , training loss: 2.23213170736 , training acc: 0.500769226979 , valid loss: 2.8511449231 , valid acc: 0.31888889273\n",
      "time taken: 2.6769111156463623\n",
      "epoch: 14 , training loss: 2.19372157256 , training acc: 0.520256405457 , valid loss: 2.85615242852 , valid acc: 0.308888890677\n",
      "time taken: 2.7313177585601807\n",
      "epoch: 15 , training loss: 2.14417085281 , training acc: 0.535128203722 , valid loss: 2.85008809302 , valid acc: 0.326666673024\n",
      "epoch: 15 , pass acc: 0.332222223282 , current acc: 0.333333330022\n",
      "time taken: 2.7335212230682373\n",
      "epoch: 16 , training loss: 2.10588773092 , training acc: 0.54794871425 , valid loss: 2.88674301571 , valid acc: 0.333333330022\n",
      "time taken: 2.696093797683716\n",
      "epoch: 17 , training loss: 2.05618769695 , training acc: 0.570512819749 , valid loss: 2.91154893239 , valid acc: 0.322222222884\n",
      "time taken: 2.7143619060516357\n",
      "epoch: 18 , training loss: 2.01474694105 , training acc: 0.5741025645 , valid loss: 2.96705102921 , valid acc: 0.314444445901\n",
      "time taken: 2.787672519683838\n",
      "epoch: 19 , training loss: 1.96917599592 , training acc: 0.595384617647 , valid loss: 3.0041364034 , valid acc: 0.312222222487\n",
      "time taken: 3.2139837741851807\n",
      "epoch: 20 , training loss: 1.89698334535 , training acc: 0.617179486996 , valid loss: 3.04811032613 , valid acc: 0.304444445504\n",
      "time taken: 3.243159770965576\n",
      "epoch: 21 , training loss: 1.86413286588 , training acc: 0.641025645611 , valid loss: 3.1082807117 , valid acc: 0.301111113694\n",
      "time taken: 3.17820405960083\n",
      "epoch: 22 , training loss: 1.78451717206 , training acc: 0.66076923028 , valid loss: 3.18033197191 , valid acc: 0.311111112436\n",
      "time taken: 3.163747549057007\n",
      "epoch: 23 , training loss: 1.72045272436 , training acc: 0.69076923835 , valid loss: 3.25780269835 , valid acc: 0.30222222209\n",
      "time taken: 3.2165491580963135\n",
      "epoch: 24 , training loss: 1.65908722388 , training acc: 0.71179487155 , valid loss: 3.30100705889 , valid acc: 0.300000003643\n",
      "time taken: 3.2288551330566406\n",
      "epoch: 25 , training loss: 1.58928593 , training acc: 0.74153845891 , valid loss: 3.41608712408 , valid acc: 0.29888889028\n",
      "time taken: 3.1561644077301025\n",
      "epoch: 26 , training loss: 1.52616654604 , training acc: 0.765897437548 , valid loss: 3.5861632559 , valid acc: 0.286666663157\n",
      "break epoch: 26\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model(num_layers, size_layer, vectors.shape[1], len(label), learning_rate)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "dimension = vectors.shape[1]\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "EARLY_STOPPING, CURRENT_CHECKPOINT, CURRENT_ACC, EPOCH = 10, 0, 0, 0\n",
    "while True:\n",
    "    lasttime = time.time()\n",
    "    if CURRENT_CHECKPOINT == EARLY_STOPPING:\n",
    "        print('break epoch:', EPOCH)\n",
    "        break\n",
    "    train_acc, train_loss, test_acc, test_loss = 0, 0, 0, 0\n",
    "    for i in range(0, (train_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = train_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "            batch_y[k, int(train_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        train_loss += loss\n",
    "        train_acc += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "    \n",
    "    for i in range(0, (test_X.shape[0] // batch) * batch, batch):\n",
    "        batch_x = np.zeros((batch, maxlen, dimension))\n",
    "        batch_y = np.zeros((batch, len(label)))\n",
    "        for k in range(batch):\n",
    "            tokens = test_X[i + k].split()[:maxlen]\n",
    "            emb_data = np.zeros((maxlen, dimension), dtype = np.float32)\n",
    "            for no, text in enumerate(tokens[::-1]):\n",
    "                try:\n",
    "                    emb_data[-1 - no, :] += vectors[dictionary[text], :]\n",
    "                except:\n",
    "                    continue\n",
    "            batch_y[k, int(test_Y[i + k])] = 1.0\n",
    "            batch_x[k, :, :] = emb_data[:, :]\n",
    "        loss, acc = sess.run([model.cost, model.accuracy], feed_dict = {model.X : batch_x, model.Y : batch_y})\n",
    "        test_loss += loss\n",
    "        test_acc += acc\n",
    "        \n",
    "    train_loss /= (train_X.shape[0] // batch)\n",
    "    train_acc /= (train_X.shape[0] // batch)\n",
    "    test_loss /= (test_X.shape[0] // batch)\n",
    "    test_acc /= (test_X.shape[0] // batch)\n",
    "    if test_acc > CURRENT_ACC:\n",
    "        print('epoch:', EPOCH, ', pass acc:', CURRENT_ACC, ', current acc:', test_acc)\n",
    "        CURRENT_ACC = test_acc\n",
    "        CURRENT_CHECKPOINT = 0\n",
    "        saver.save(sess, os.getcwd() + \"/rnn-message.ckpt\")\n",
    "    else:\n",
    "        CURRENT_CHECKPOINT += 1\n",
    "    EPOCH += 1\n",
    "    print('time taken:', time.time()-lasttime)\n",
    "    print('epoch:', EPOCH, ', training loss:', train_loss, ', training acc:', train_acc, ', valid loss:', test_loss, ', valid acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
